{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpAKL4fT9IVy"
      },
      "source": [
        "This notebook demonstrates a simple implementation of the MultiDecode algorithm.  In this notebook the attention masks and position_ids are generated manually and then passed to mdgen for iterative token generation.  A few simple examples are demonstrated.\n",
        "- beam search\n",
        "- parallel questions\n",
        "- writing in the margins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6DtlTMZr9IVz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "\n",
        "import copy\n",
        "import time\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM,AutoConfig\n",
        "import torch\n",
        "import transformers\n",
        "import torch.nn.functional as F\n",
        "from functools import partial\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "bKftGKTu9IV0"
      },
      "outputs": [],
      "source": [
        "# Log in to Hugging Face\n",
        "\n",
        "from huggingface_hub import login\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    hf_token=userdata.get('huggingface')\n",
        "except:\n",
        "    import os\n",
        "    import dotenv\n",
        "    dotenv.load_dotenv(\"../.env\")\n",
        "    hf_token=os.getenv('HUGGINGFACE')\n",
        "\n",
        "\n",
        "login(token=hf_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dsr6U0zB9IV0"
      },
      "outputs": [],
      "source": [
        "def mdgen(model, input_ids,positions=None,mask=None,gen_len=10,n_branch=2,greedy=False,branch_locations=None,past_key_values=None):\n",
        "    \"\"\"\n",
        "    Implements the parallel generation of tokens using the multidecode technique.\n",
        "\n",
        "    This function generates tokens in parallel by branching at specified positions in the input sequence.\n",
        "    It uses a model's forward pass to compute logits and generate tokens iteratively, either greedily or\n",
        "    through sampling. The generated tokens are accumulated and returned along with other relevant data.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model used for token generation. Must support `forward` with caching.\n",
        "        input_ids (torch.Tensor): Input token IDs of shape (batch_size, ctx_len).\n",
        "        positions (torch.Tensor, optional): Position encodings for the input tokens. If None, defaults to\n",
        "            sequential positions [0, 1, ..., ctx_len-1]. Shape must match `input_ids`.\n",
        "        mask (torch.Tensor): Attention mask of shape (batch_size, ctx_len, ctx_len). Controls which tokens\n",
        "            the model attends to during prefill.\n",
        "        gen_len (int, optional): Number of tokens to generate. Defaults to 10.\n",
        "        n_branch (int, optional): Number of parallel branches for token generation. Defaults to 2.\n",
        "        greedy (bool, optional): If True, selects the most probable token at each step. If False, samples\n",
        "            tokens based on probabilities. Defaults to False.\n",
        "        branch_locations (list, optional): List of positions where branches start. If None, defaults to\n",
        "            the end of the input context.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing:\n",
        "            - 'branch_ids' (torch.Tensor): Generated token IDs for each branch, reshaped to (n_branch, batch_size).\n",
        "            - 'mask' (torch.Tensor): Final attention mask after generation.\n",
        "            - 'output_ids' (torch.Tensor): All generated token IDs concatenated sequentially.\n",
        "            - 'input_ids' (torch.Tensor): Original input token IDs.\n",
        "            - 'positions' (torch.Tensor): Position encodings for all tokens, including generated ones.\n",
        "\n",
        "    Raises:\n",
        "        AssertionError: If `positions` shape does not match `input_ids` shape.\n",
        "\n",
        "    Example:\n",
        "        output = mdgen(\n",
        "            model=my_model,\n",
        "            input_ids=torch.tensor([[1, 2, 3]]),\n",
        "            mask=torch.ones(1, 3, 3),\n",
        "            gen_len=5,\n",
        "            n_branch=2,\n",
        "            greedy=True\n",
        "        )\n",
        "        print(output['branch_ids'])\n",
        "    \"\"\"\n",
        "\n",
        "    past_len=0 if past_key_values is None else past_key_values.get_seq_length()\n",
        "\n",
        "    if positions is not None:\n",
        "        assert input_ids.shape == positions.shape,\"positions.shape must match input_ids.shape\"\n",
        "    #assert mask.shape[2]==input_ids.shape[1],\"length of attn mask must match input length\"\n",
        "\n",
        "\n",
        "    batch_size,ctx_len=input_ids.shape\n",
        "\n",
        "    # every cycle we add n_branch more tokens, so the end of the 4D attention mask is a diagonal. Create here and reuse below\n",
        "    gen_mask = torch.where(torch.eye(n_branch) == 1, torch.tensor(0.0), torch.tensor(float('-inf'))).unsqueeze(0).unsqueeze(0).to(model.device)\n",
        "\n",
        "    # position information of the initial context input_ids. If None assume 0..ctx_len\n",
        "    if positions is None:\n",
        "        positions=torch.arange(ctx_len,dtype=torch.int).unsqueeze(0)\n",
        "    positions=positions.to(model.device)\n",
        "    position_history=copy.copy(positions)\n",
        "\n",
        "\n",
        "    # if branch location is not specified, assume all branches start at the end of the context\n",
        "    if branch_locations is None:\n",
        "        branch_locations=[ctx_len-1]*n_branch\n",
        "\n",
        "    assert all(bl>past_len for bl in branch_locations),\"Branches must start with new input_ids, not from past_key_values.\"\n",
        "\n",
        "    # the position encoding of the first generated token is just after the branch location position encoding\n",
        "    tmp=[int(positions[0,x]) for x in branch_locations]\n",
        "    gen_positions=torch.tensor(tmp).unsqueeze(0).to(model.device)\n",
        "\n",
        "\n",
        "    # we will accumulate the generated tokens into output_ids\n",
        "    output_ids=torch.empty((batch_size,0),dtype=torch.int).to(model.device)\n",
        "\n",
        "    # move remaining tensors to model.device\n",
        "    mask=mask.to(model.device)\n",
        "    input_ids=input_ids.to(model.device)\n",
        "    initial_length=input_ids.shape[1]\n",
        "    pkv=past_key_values\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # first step is to prefill the context and generate pkv\n",
        "        output=model.forward(input_ids=input_ids[:,past_len:],position_ids=positions[:,past_len:] ,attention_mask=mask, use_cache=True,past_key_values=pkv)\n",
        "        pkv = output.past_key_values\n",
        "\n",
        "        # get logits from the locations where the branches fork\n",
        "        branch_locations=torch.tensor(branch_locations,dtype=torch.int)\n",
        "\n",
        "        # branch locations are relative to full input sequence,\n",
        "        # so we subtrack the pkv length\n",
        "        logits=output['logits'][:,branch_locations-past_len,:]\n",
        "        mask = mask[:,:,branch_locations-past_len,:]\n",
        "\n",
        "        for i in range(gen_len):\n",
        "            # select tokens, greedy or not\n",
        "            next_token_probs = F.softmax(logits / 0.7, dim=-1)\n",
        "            if greedy:\n",
        "                tokens = torch.argmax(next_token_probs,dim=-1)\n",
        "            else:\n",
        "                samples = torch.multinomial(next_token_probs.view(-1,next_token_probs.shape[-1]), num_samples=1, replacement=True).view(batch_size,n_branch)\n",
        "                tokens = samples.squeeze(-1)\n",
        "\n",
        "            # save the generated tokens\n",
        "            output_ids=torch.cat([output_ids,tokens],dim=-1)\n",
        "            mask=torch.cat([mask,gen_mask],dim=-1)\n",
        "\n",
        "            # Generate n_branch new tokens.\n",
        "            output=model.forward(input_ids=tokens,position_ids=gen_positions ,attention_mask=mask, past_key_values=pkv, use_cache=True)\n",
        "            logits=output['logits']\n",
        "            pkv = output['past_key_values']\n",
        "\n",
        "            # increment the position information for the next token\n",
        "            gen_positions+=1\n",
        "\n",
        "            position_history=torch.cat([position_history,gen_positions],dim=-1)\n",
        "\n",
        "    # restruture the results to have n_branch sequences\n",
        "    branch_ids=output_ids.view(-1,n_branch,1).permute(2,1,0).squeeze(-1)\n",
        "    full_ids=torch.cat([input_ids,output_ids],dim=-1)\n",
        "    return {'branch_ids':branch_ids,'mask':mask,'output_ids':output_ids,'input_ids':full_ids,\n",
        "            'n_branch':n_branch,'initial_length':initial_length,'positions':position_history,'past_key_values':pkv}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ihHIB0289IV1"
      },
      "outputs": [],
      "source": [
        "#Helpful utilities\n",
        "def print_branches(branch_ids):\n",
        "    branch_ids=branch_ids.cpu()\n",
        "    for sidx,branches in enumerate(branch_ids):\n",
        "        for bidx,branch_ids in enumerate(branches):\n",
        "            ids=branch_ids\n",
        "            print(f\"{sidx}.{bidx}: {''.join(tokenizer.batch_decode(ids, skip_special_tokens=True))}\")\n",
        "\n",
        "def print_mask(mask):\n",
        "    for i in range(mask.shape[2]):\n",
        "        for j in range(mask.shape[3]):\n",
        "            print('*' if mask[0,0,i,j]==0. else '.',end=\"\")\n",
        "        print()\n",
        "\n",
        "def print_full(output):\n",
        "    full_ids=torch.cat([output['input_ids'],output['output_ids']],dim=-1)\n",
        "    # print(f\"{full_ids=}\")\n",
        "    # print(''.join(tokenizer.batch_decode(full_ids, skip_special_tokens=True)))\n",
        "    mask=output['mask']\n",
        "    for b in range(mask.shape[2]):\n",
        "        branch_full_ids=[]\n",
        "        for p in range(mask.shape[3]):\n",
        "            if mask[0,0,b,p] == 0.0:\n",
        "                branch_full_ids.append(int(full_ids[0,p]))\n",
        "        print(f\"{b}:{''.join(tokenizer.batch_decode(branch_full_ids, skip_special_tokens=True))}\")\n",
        "\n",
        "\n",
        "def print_args(input_ids=None,positions=None,mask=None,branch_locations=None):\n",
        "    print()\n",
        "    print(\"Arguments:\")\n",
        "    print(f\"{input_ids.shape=}\")\n",
        "    if positions is not None:\n",
        "        print(f\"{positions=}\")\n",
        "    if branch_locations is not None:\n",
        "        print(f\"{branch_locations=}\")\n",
        "    print()\n",
        "    if mask is not None:\n",
        "        print_mask(mask)\n",
        "    print()\n",
        "\n",
        "def print_results(output):\n",
        "    print()\n",
        "    print(\"Results\")\n",
        "    print(\"raw\")\n",
        "    print(f\"{''.join(tokenizer.batch_decode(output['output_ids'], skip_special_tokens=True))}\")\n",
        "    print()\n",
        "    print(\"Reformated\")\n",
        "    print_full(output)\n",
        "    print()\n",
        "    print(f\"positions {output['positions']}\")\n",
        "    print()\n",
        "    print_mask(output['mask'])\n",
        "    print()\n",
        "\n",
        "def strmask(*args):\n",
        "    n_branch=len(args)\n",
        "    seq_len=len(args[0])\n",
        "    ret=torch.full((n_branch,seq_len),fill_value=float('-inf'))\n",
        "\n",
        "    for b,arg in enumerate(args):\n",
        "        for i,v in enumerate(arg):\n",
        "            if v=='1' or v=='*':\n",
        "                ret[b,i]=0\n",
        "    return ret.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "def lut_attn(n):\n",
        "    ''''\n",
        "\n",
        "    Returns a lower triangle array with dimensions and values suitable for an attention mask\n",
        "    dimension: [1,1,n,n]\n",
        "    values: 0 in lower triangle and diagonal\n",
        "            -inf in upper triangle\n",
        "\n",
        "    '''\n",
        "    return torch.where(torch.tril(torch.ones(n,n)) == 1, torch.tensor(0.0), torch.tensor(float('-inf'))).unsqueeze(0).unsqueeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MIwY5gyv9IV2"
      },
      "outputs": [],
      "source": [
        "#Initialize model\n",
        "model_name=\"meta-llama/Llama-3.2-1B\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name,padding_side='left')\n",
        "tokenizer.pad_token_id=tokenizer.eos_token_id\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)#,attn_implementation=\"flex_attention\")\n",
        "model = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Use GPU if available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZe80UnJ9IV2",
        "outputId": "fbe80c73-cbc6-45e6-d6b3-0ede9af9a3ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Arguments:\n",
            "input_ids.shape=torch.Size([1, 5])\n",
            "\n",
            "*....\n",
            "**...\n",
            "***..\n",
            "****.\n",
            "*****\n",
            "\n",
            "\n",
            "Results\n",
            "raw\n",
            ", in,, there there the there an was were German was idea a twoic a was little companion kingdoms man born boy animals there called in who who was John the lived were a. heart in more king He of the than who had a heart\n",
            "\n",
            "Reformated\n",
            "0:Once upon a time, there were two companion animals who were more than\n",
            "1:Once upon a time in the Germanic kingdoms there was a king who\n",
            "2:Once upon a time, there was a man called John. He had\n",
            "3:Once upon a time, an idea was born in the heart of a\n",
            "4:Once upon a time there was a little boy who lived in the heart\n",
            "\n",
            "positions tensor([[ 0,  1,  2,  3,  4,  5,  5,  5,  5,  5,  6,  6,  6,  6,  6,  7,  7,  7,\n",
            "          7,  7,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 11,\n",
            "         11, 11, 11, 11, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 14, 14, 14, 14,\n",
            "         14]], device='cuda:0')\n",
            "\n",
            "******....*....*....*....*....*....*....*....*....*....\n",
            "*****.*....*....*....*....*....*....*....*....*....*...\n",
            "*****..*....*....*....*....*....*....*....*....*....*..\n",
            "*****...*....*....*....*....*....*....*....*....*....*.\n",
            "*****....*....*....*....*....*....*....*....*....*....*\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# simple beam generation\n",
        "input_ids=tokenizer(\"Once upon a time\", return_tensors=\"pt\", padding=True, truncation=True)['input_ids'].to(model.device)\n",
        "\n",
        "mask=lut_attn(input_ids.shape[1])\n",
        "\n",
        "print_args(input_ids,mask=mask)\n",
        "\n",
        "output=mdgen(model, input_ids,positions=None,mask=mask,n_branch=5,greedy=False)\n",
        "\n",
        "print_results(output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXDazZl09IV3",
        "outputId": "11db450d-b81c-46c3-e8d0-3e3db5d3f570"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Arguments:\n",
            "input_ids.shape=torch.Size([1, 31])\n",
            "positions=tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
            "         18, 19, 20, 21, 22, 23, 17, 18, 19, 20, 21, 22, 23]])\n",
            "\n",
            "*..............................\n",
            "**.............................\n",
            "***............................\n",
            "****...........................\n",
            "*****..........................\n",
            "******.........................\n",
            "*******........................\n",
            "********.......................\n",
            "*********......................\n",
            "**********.....................\n",
            "***********....................\n",
            "************...................\n",
            "*************..................\n",
            "**************.................\n",
            "***************................\n",
            "****************...............\n",
            "*****************..............\n",
            "******************.............\n",
            "*******************............\n",
            "********************...........\n",
            "*********************..........\n",
            "**********************.........\n",
            "***********************........\n",
            "************************.......\n",
            "*****************.......*......\n",
            "*****************.......**.....\n",
            "*****************.......***....\n",
            "*****************.......****...\n",
            "*****************.......*****..\n",
            "*****************.......******.\n",
            "*****************.......*******\n",
            "\n",
            "\n",
            "Results\n",
            "raw\n",
            " The The answer answer is is purple green.. The The bike grass is is purple green..\n",
            "\n",
            "Reformated\n",
            "0:The house is red. The grass is green. The bike is purple. What color is the bike? The answer is purple. The bike is purple.\n",
            "1:The house is red. The grass is green. The bike is purple. What color is the grass? The answer is green. The grass is green.\n",
            "\n",
            "positions tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
            "         18, 19, 20, 21, 22, 23, 17, 18, 19, 20, 21, 22, 23, 24, 24, 25, 25, 26,\n",
            "         26, 27, 27, 28, 28, 29, 29, 30, 30, 31, 31, 32, 32, 33, 33]],\n",
            "       device='cuda:0')\n",
            "\n",
            "************************.......*.*.*.*.*.*.*.*.*.*.\n",
            "*****************.......*******.*.*.*.*.*.*.*.*.*.*\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Multi-question\n",
        "context_ids=tokenizer(\"The house is red. The grass is green. The bike is purple. \", return_tensors=\"pt\", padding=True, truncation=True)['input_ids'].to(model.device)\n",
        "question1_ids=tokenizer(\"What color is the bike?\", return_tensors=\"pt\", padding=True, truncation=True)['input_ids'].to(model.device)\n",
        "question2_ids=tokenizer(\"What color is the grass?\", return_tensors=\"pt\", padding=True, truncation=True)['input_ids'].to(model.device)\n",
        "context_len=context_ids.shape[1]\n",
        "question1_len=question1_ids.shape[1]\n",
        "question2_len=question2_ids.shape[1]\n",
        "\n",
        "input_ids=torch.cat([context_ids,question1_ids,question2_ids],dim=-1)\n",
        "\n",
        "mask=lut_attn(input_ids.shape[1])\n",
        "# mask out the first question from the view of the second question\n",
        "mask[:,:,context_len+question1_len:,context_len:context_len+question1_len]=float('-inf')\n",
        "\n",
        "positions=torch.cat([torch.arange(context_len+question1_len),torch.arange(context_len,context_len+question2_len)]).unsqueeze(0)\n",
        "branch_locations=[context_len+question1_len-1,context_len+question1_len+question2_len-1]\n",
        "\n",
        "print_args(input_ids,positions,mask)\n",
        "\n",
        "output=mdgen(model, input_ids,positions=positions,mask=mask,branch_locations=branch_locations,greedy=True)\n",
        "\n",
        "print_results(output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0_Zaz-E9IV4",
        "outputId": "5a617f51-a6d9-4e83-cc71-c7457aa2ded8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "context1_len=7 context2_len=12 question1_len=7 question2_len=7\n",
            "positions=tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
            "         18,  7,  8,  9, 10, 11, 12, 13, 19, 20, 21, 22, 23, 24, 25]])\n",
            "input_ids.shape=torch.Size([1, 33]) positions.shape=torch.Size([1, 33])\n",
            "\n",
            "Arguments:\n",
            "input_ids.shape=torch.Size([1, 33])\n",
            "positions=tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
            "         18,  7,  8,  9, 10, 11, 12, 13, 19, 20, 21, 22, 23, 24, 25]])\n",
            "\n",
            "*................................\n",
            "**...............................\n",
            "***..............................\n",
            "****.............................\n",
            "*****............................\n",
            "******...........................\n",
            "*******..........................\n",
            "********.........................\n",
            "*********........................\n",
            "**********.......................\n",
            "***********......................\n",
            "************.....................\n",
            "*************....................\n",
            "**************...................\n",
            "***************..................\n",
            "****************.................\n",
            "*****************................\n",
            "******************...............\n",
            "*******************..............\n",
            "*******............*.............\n",
            "*******............**............\n",
            "*******............***...........\n",
            "*******............****..........\n",
            "*******............*****.........\n",
            "*******............******........\n",
            "*******............*******.......\n",
            "*******************.......*......\n",
            "*******************.......**.....\n",
            "*******************.......***....\n",
            "*******************.......****...\n",
            "*******************.......*****..\n",
            "*******************.......******.\n",
            "*******************.......*******\n",
            "\n",
            "\n",
            "Results\n",
            "raw\n",
            " The The house answer was is red blue..  The2 house. is What red color.\n",
            "\n",
            "Reformated\n",
            "0:The house was red. What color is the bike? The house was red. 2. What color\n",
            "1:The house was red. The grass was green. The bike was blue. What color is the bike? The answer is blue. The house is red.\n",
            "\n",
            "positions tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
            "         18,  7,  8,  9, 10, 11, 12, 13, 19, 20, 21, 22, 23, 24, 25, 14, 26, 15,\n",
            "         27, 16, 28, 17, 29, 18, 30, 19, 31, 20, 32, 21, 33, 22, 34, 23, 35]],\n",
            "       device='cuda:0')\n",
            "\n",
            "*******............*******.......*.*.*.*.*.*.*.*.*.*.\n",
            "*******************.......*******.*.*.*.*.*.*.*.*.*.*\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Writing in the margins\n",
        "context1_ids=tokenizer(\"The house was red. \", return_tensors=\"pt\", padding=True, truncation=True)['input_ids'].to(model.device)\n",
        "context2_ids=tokenizer(\"The grass was green. The bike was blue. \", return_tensors=\"pt\", padding=True, truncation=True)['input_ids'].to(model.device)\n",
        "question1_ids=tokenizer(\"What color is the bike?\", return_tensors=\"pt\", padding=True, truncation=True)['input_ids'].to(model.device)\n",
        "question2_ids=tokenizer(\"What color is the bike?\", return_tensors=\"pt\", padding=True, truncation=True)['input_ids'].to(model.device)\n",
        "context1_len=context1_ids.shape[1]\n",
        "context2_len=context2_ids.shape[1]\n",
        "question1_len=question1_ids.shape[1]\n",
        "question2_len=question2_ids.shape[1]\n",
        "print(f\"{context1_len=} {context2_len=} {question1_len=} {question2_len=}\")\n",
        "\n",
        "input_ids=torch.cat([context1_ids,context2_ids,question1_ids,question2_ids],dim=-1)\n",
        "\n",
        "# mask out the second context from the first question, and the first question from the view of the second question\n",
        "mask=lut_attn(input_ids.shape[1])\n",
        "mask[:,:,context1_len+context2_len:context1_len+context2_len+question1_len,context1_len:context1_len+context2_len]=float('-inf')\n",
        "mask[:,:,context1_len+context2_len+question1_len:,context1_len+context2_len:context1_len+context2_len+question1_len]=float('-inf')\n",
        "\n",
        "\n",
        "# make question 1 follow context1 and question 2 follow context2\n",
        "positions=torch.cat([\n",
        "    torch.arange(context1_len+context2_len),\n",
        "    torch.arange(context1_len,context1_len+question1_len),\n",
        "    torch.arange(context1_len+context2_len,context1_len+context2_len+question2_len)]).unsqueeze(0)\n",
        "\n",
        "print(f\"{positions=}\")\n",
        "print(f\"{input_ids.shape=} {positions.shape=}\")\n",
        "branch_locations=[context1_len+context2_len+question1_len-1,context1_len+context2_len+question1_len+question2_len-1]\n",
        "\n",
        "print_args(input_ids,positions,mask)\n",
        "\n",
        "output=mdgen(model, input_ids,positions=positions,mask=mask,branch_locations=branch_locations,greedy=True)\n",
        "\n",
        "print_results(output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-VH3gei9IV4",
        "outputId": "3ba1f326-d80b-4e03-d905-002860ae7cb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Arguments:\n",
            "input_ids.shape=torch.Size([1, 6])\n",
            "\n",
            "*.....\n",
            "**....\n",
            "***...\n",
            "****..\n",
            "*****.\n",
            "******\n",
            "\n",
            "\n",
            "Results\n",
            "raw\n",
            " 2\n",
            "\n",
            "Reformated\n",
            "0:5 - 3 = 2\n",
            "\n",
            "positions tensor([[0, 1, 2, 3, 4, 5, 6, 7]], device='cuda:0')\n",
            "\n",
            "********\n",
            "\n",
            "0:5 - 3 = 2\n"
          ]
        }
      ],
      "source": [
        "# Teds example part 1\n",
        "input_ids=tokenizer(\"5 - 3 =\", return_tensors=\"pt\", padding=True, truncation=True)['input_ids'].to(model.device)\n",
        "\n",
        "mask=lut_attn(input_ids.shape[1])\n",
        "\n",
        "print_args(input_ids,mask=mask)\n",
        "\n",
        "output=mdgen(model, input_ids,mask=mask,n_branch=1,gen_len=2,greedy=True)\n",
        "\n",
        "print_results(output)\n",
        "print_full(output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWPBbggS9IV4",
        "outputId": "946bc21e-7ec3-496d-e134-4b55adab3c1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original_ids.cpu()=tensor([[128000,     20,    482,    220,     18,    284]])\n",
            "input_ids.cpu()=tensor([[128000,     20,    284,    482,    220,     18]])\n",
            "\n",
            "Arguments:\n",
            "input_ids.shape=torch.Size([1, 6])\n",
            "positions=tensor([[0, 1, 5, 2, 3, 4]], dtype=torch.int32)\n",
            "\n",
            "*.....\n",
            "**....\n",
            "******\n",
            "**.*..\n",
            "**.**.\n",
            "**.***\n",
            "\n",
            "\n",
            "Results\n",
            "raw\n",
            " 2\n",
            "\n",
            "Reformated\n",
            "0:5 = - 3 2\n",
            "\n",
            "positions tensor([[0, 1, 5, 2, 3, 4, 6, 7]], device='cuda:0')\n",
            "\n",
            "********\n",
            "\n",
            "0:5 = - 3 2\n"
          ]
        }
      ],
      "source": [
        "# Teds example part 2\n",
        "original_ids=tokenizer(\"5 - 3 =\", return_tensors=\"pt\", padding=True, truncation=True)['input_ids'].to(model.device)\n",
        "print(f\"{original_ids.cpu()=}\")\n",
        "input_ids=tokenizer(\"5 = - 3\", return_tensors=\"pt\", padding=True, truncation=True)['input_ids'].to(model.device)\n",
        "print(f\"{input_ids.cpu()=}\")\n",
        "\n",
        "\n",
        "mask=strmask(\"*.....\",\"**....\",\"******\",\"**.*..\",\"**.**.\",\"**.***\")\n",
        "\n",
        "order=torch.tensor([0, 1, 3, 4, 5, 2])\n",
        "\n",
        "positions=torch.tensor([[0, 1, 5, 2,3,4]],dtype=torch.int)\n",
        "branch_locations=[2]\n",
        "\n",
        "print_args(input_ids,mask=mask,positions=positions)\n",
        "\n",
        "output=mdgen(model, input_ids,positions=positions,mask=mask,branch_locations=branch_locations,n_branch=1,gen_len=2,greedy=True)\n",
        "\n",
        "print_results(output)\n",
        "print_full(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "fZBlTElX9IV4"
      },
      "outputs": [],
      "source": [
        "def select_branch(output,selected_branch):\n",
        "    \"\"\"\n",
        "    Selects a specific branch from the output of the `mdgen` function.\n",
        "\n",
        "    This function extracts the input IDs, position IDs, attention mask, and past key values\n",
        "    corresponding to the specified branch index from the output of the `mdgen` function.\n",
        "\n",
        "    Args:\n",
        "        output (dict): The output dictionary from the `mdgen` function. It should contain:\n",
        "            - 'branch_ids' (torch.Tensor): Generated token IDs for each branch.\n",
        "            - 'positions' (torch.Tensor): Position encodings for each branch.\n",
        "            - 'mask' (torch.Tensor): Attention mask for each branch.\n",
        "            - 'past_key_values' (optional): Cached key-value pairs for efficient decoding.\n",
        "        branch_index (int): The index of the branch to select.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - input2_ids (torch.Tensor): The token IDs for the selected branch.\n",
        "            - position2_ids (torch.Tensor): The position encodings for the selected branch.\n",
        "            - mask2 (torch.Tensor): The attention mask for the selected branch.\n",
        "            - pkv (optional): The past key-value pairs for the selected branch, if available.\n",
        "\n",
        "    Raises:\n",
        "        IndexError: If the specified branch index is out of range.\n",
        "\n",
        "    Example:\n",
        "        input2_ids, position2_ids, mask2, pkv = select_branch(output, branch_index=1)\n",
        "    \"\"\"\n",
        "    pkv=output['past_key_values']\n",
        "    o_positions=output['positions']\n",
        "    o_mask=output['mask']\n",
        "    o_input_ids=output['input_ids']\n",
        "    o_initial_len=output['initial_length']\n",
        "    o_input_ids_len=o_input_ids.shape[1]\n",
        "\n",
        "    input_indexes=torch.cat([torch.arange(o_initial_len),torch.arange(o_initial_len+selected_branch,o_input_ids_len+1,n_branch,dtype=torch.int)],dim=-1)\n",
        "\n",
        "    input2_ids=o_input_ids[:,input_indexes]\n",
        "    position2_ids=o_positions[:,input_indexes]\n",
        "    selected_len=(o_input_ids.shape[1]- o_initial_len)//n_branch\n",
        "    mask2=o_mask[:,:,selected_branch,input_indexes[:o_initial_len]].repeat([1,1,selected_len,1])\n",
        "    mask2=torch.cat([mask2,lut_attn(selected_len).to(model.device)],dim=-1)\n",
        "\n",
        "    pkv.crop(o_initial_len)\n",
        "\n",
        "    print(f\"{input2_ids.shape=} {position2_ids.shape=} {mask2.shape=} {pkv.get_seq_length()=}\")\n",
        "    return input2_ids,position2_ids, mask2, pkv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKN0r09S9IV5",
        "outputId": "2b18afd3-5a46-494f-8015-0d2193fc3120"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Arguments:\n",
            "input_ids.shape=torch.Size([1, 5])\n",
            "\n",
            "*....\n",
            "**...\n",
            "***..\n",
            "****.\n",
            "*****\n",
            "\n",
            "\n",
            "Results\n",
            "raw\n",
            ",,, there there there was was lived a a a boy small happy who island, lived, peaceful in a and a small very small country productive\n",
            "\n",
            "Reformated\n",
            "0:Once upon a time, there was a boy who lived in a small\n",
            "1:Once upon a time, there was a small island, a small country\n",
            "2:Once upon a time, there lived a happy, peaceful and very productive\n",
            "\n",
            "positions tensor([[ 0,  1,  2,  3,  4,  5,  5,  5,  6,  6,  6,  7,  7,  7,  8,  8,  8,  9,\n",
            "          9,  9, 10, 10, 10, 11, 11, 11, 12, 12, 12, 13, 13, 13, 14, 14, 14]],\n",
            "       device='cuda:0')\n",
            "\n",
            "******..*..*..*..*..*..*..*..*..*..\n",
            "*****.*..*..*..*..*..*..*..*..*..*.\n",
            "*****..*..*..*..*..*..*..*..*..*..*\n",
            "\n",
            "input2_ids.shape=torch.Size([1, 15]) position2_ids.shape=torch.Size([1, 15]) mask2.shape=torch.Size([1, 1, 10, 15]) pkv.get_seq_length()=5\n",
            "\n",
            "Arguments:\n",
            "input_ids.shape=torch.Size([1, 15])\n",
            "positions=tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14]],\n",
            "       device='cuda:0')\n",
            "\n",
            "******.........\n",
            "*******........\n",
            "********.......\n",
            "*********......\n",
            "**********.....\n",
            "***********....\n",
            "************...\n",
            "*************..\n",
            "**************.\n",
            "***************\n",
            "\n",
            "\n",
            "Results\n",
            "raw\n",
            ", where called a there Iceland tiny was. nation no The. war people The, of island where Iceland’s people lived name gathered ordinary was, lives\n",
            "\n",
            "Reformated\n",
            "0:Once upon a time, there was a small island, a small country, a tiny nation. The island’s name was\n",
            "1:Once upon a time, there was a small island, a small country where there was no war, where people gathered,\n",
            "2:Once upon a time, there was a small island, a small country called Iceland. The people of Iceland lived ordinary lives\n",
            "\n",
            "positions tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 15, 15,\n",
            "         16, 16, 16, 17, 17, 17, 18, 18, 18, 19, 19, 19, 20, 20, 20, 21, 21, 21,\n",
            "         22, 22, 22, 23, 23, 23, 24, 24, 24]], device='cuda:0')\n",
            "\n",
            "****************..*..*..*..*..*..*..*..*..*..\n",
            "***************.*..*..*..*..*..*..*..*..*..*.\n",
            "***************..*..*..*..*..*..*..*..*..*..*\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Example for using the select branch function\n",
        "\n",
        "input_ids=tokenizer(\"Once upon a time\", return_tensors=\"pt\", padding=True, truncation=True)['input_ids'].to(model.device)\n",
        "mask=lut_attn(input_ids.shape[1])\n",
        "\n",
        "print_args(input_ids,mask=mask)\n",
        "\n",
        "input_len=input_ids.shape[1]\n",
        "n_branch=3\n",
        "output=mdgen(model, input_ids,positions=None,mask=mask,n_branch=n_branch,greedy=False)\n",
        "print_results(output)\n",
        "selected_branch=1\n",
        "\n",
        "\n",
        "\n",
        "input2_ids,position2_ids,mask2, pkv=select_branch(output,selected_branch)\n",
        "\n",
        "print_args(input2_ids,positions=position2_ids,mask=mask2)\n",
        "\n",
        "output=mdgen(model, input2_ids,positions=position2_ids,mask=mask2,n_branch=n_branch,greedy=False,past_key_values=pkv)\n",
        "\n",
        "print_results(output)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "history_visible": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}